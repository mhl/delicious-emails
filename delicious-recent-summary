#!/usr/bin/env python

# This script produces a plain text (UTF-8 encoded) summary of recent
# links posted to delicious.com by the users listed (one per line) in
# the file specified as the first argument.  You can either report all
# those in the last 7 days, or, if <NUMBER-TRIGGER> is supplied,
# produced empty output unless more than <NUMBER-TRIGGER> links have
# been added since the last time the script did produce output in this
# mode.

# Copyright Mark Longair 2008, 2009, 2010

import feedparser
import textwrap
import datetime
from htmlentitydefs import name2codepoint
import re
from pysqlite2 import dbapi2 as sqlite
import os
import sys
import json
import urllib2
from cookielib import CookieJar
from urlparse import urlparse
from lxml import etree
from collections import defaultdict

# We need a cookie jar, or opening t.co links results in an infinite
# 302 redirection loop:
cj = CookieJar()
url_opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))

# An sqlite database that keeps track of which links have already been
# reported, and when the triggering number of new links was last hit:
db_filename = os.path.join( os.environ['HOME'], "delicious-links.db" )

connection = sqlite.connect(db_filename)
cursor = connection.cursor()

cursor.execute("CREATE TABLE IF NOT EXISTS links ( url text, user text, updated_at timestamp )")
cursor.execute("CREATE TABLE IF NOT EXISTS updates ( last_sent_at timestamp )")

def usage():
    print '''Usage:

  get-nt-delicious <USER LIST FILE>  [NUMBER TRIGGER]

<USER LIST FILE> should be a file which lists one delicious.com
username on each line.

If [NUMBER TRIGGER] is specified, only produce output if there
have been more than that number of new bookmarks since the
last time such a limit was exceeded.'''

number_trigger = None

if len(sys.argv) == 2:
    pass
elif len(sys.argv) == 3:
    number_trigger = int(sys.argv[2],10)
else:
    usage()
    sys.exit(-1)

user_filename = sys.argv[1]

users = []
fp = open(user_filename)
for line in fp:
    stripped = line.strip()
    if len(stripped) > 0:
        users.append(stripped)
fp.close()

# Only report bookmarks that have been updated since 'since'.  By
# default this is seven days ...
since = datetime.datetime.now() - datetime.timedelta(days=7)

if number_trigger:
    # ... or if we're using a number trigger, make it the last time
    # the trigger was reached:
    cursor.execute("SELECT last_sent_at FROM updates ORDER BY last_sent_at DESC LIMIT 1")
    row = cursor.fetchone()
    if row:
        since = datetime.datetime.strptime( row[0], "%Y-%m-%d %H:%M:%S" )

def update_last_sent():
    cursor.execute("INSERT INTO updates (last_sent_at) VALUES (CURRENT_TIMESTAMP)")
    connection.commit()

# These next two useful functions are from:
#   http://snippets.dzone.com/posts/show/4569

def substitute_entity(match):
    ent = match.group(2)
    if match.group(1) == "#":
        return unichr(int(ent))
    else:
        cp = name2codepoint.get(ent)
        if cp:
            return unichr(cp)
        else:
            return match.group()

def decode_htmlentities(string):
    entity_re = re.compile("&(#?)(\d{1,5}|\w{1,8});")
    return entity_re.subn(substitute_entity, string)[0]

class DeliciousFeed:
    def __init__(self,user):
        self.user = user
        self.feed_url = "http://feeds.delicious.com/v2/rss/%s?count=100&plain" % user
        self.parsed = feedparser.parse(self.feed_url)
    def __str__(self):
        return self.feed_url
    # Add each entry to the global URL dictionary:
    def add_entries_to_url_dictionary(self,since,dictionary):
        d = dictionary
        for e in self.parsed['entries']:
            if not e['updated_parsed']:
               print >> sys.stderr, "An entry in %s had no updated_parsed field - pubDate might be empty" % (self.feed_url)
               continue
            updated = datetime.datetime(*(e['updated_parsed'][0:7]))
            if updated > since:
                url = e['link']
                summary = e.get('summary','')
                tags = None
                if 'tags' in e:
                    tags = [ x['term'] for x in e['tags'] if x['term'] and not re.search('^system:',x['term']) ]
                # For some reason I don't understand, sometimes I get
                # errors about there being no entry for 'author':
                author = e.get('author','[author missing]')
                title = e.get('title','[title missing]')
                d[url].append( SimpleEntry(author,url,title,summary,updated,tags ) )
                d[url].sort( key=lambda se: se.updated )

class SimpleEntry:
    def __init__(self,user,url,title,summary,updated,tags):
        self.user = user
        self.url = url
        self.title = title
        self.summary = summary
        self.updated = updated
        self.tags = tags

url_dictionary = defaultdict(list)

for u in users:
    feed = DeliciousFeed(u)
    feed.add_entries_to_url_dictionary(since,url_dictionary)

# Also get Francis's ScraperWiki JSON of N&T people's tweets that
# contain URLs and aren't replies:

twitter_json_url = 'https://api.scraperwiki.com/api/1.0/datastore/sqlite?format=json&name=notes_and_theories_twitter_scraper&query=select+*+from+`swdata`&apikey='

f = url_opener.open(twitter_json_url)
table = json.load(f)
f.close()

for row in table:
    created_at = datetime.datetime.strptime(row['created_at'], '%Y-%m-%dT%H:%M:%S')
    if created_at < since:
        continue
    text = row['text'].strip()
    text = re.sub('\s+', ' ', text)
    first_url_match = re.search('https?://\S+', text)
    if not first_url_match:
        print >> sys.stderr, "Failed to find a URL in: ", text
        continue
    original_url = first_url_match.group(0)
    # If there's a trailing close-banana, then probably someone's just
    # put a URL in parentheses.  Although if there's an open-banana
    # earlier in the URL  then it's probably intended to be part of the URL:
    if not '(' in original_url:
        original_url = original_url.rstrip(')')
    parser = etree.HTMLParser()
    try:
        f = url_opener.open(original_url)
        tree = etree.parse(f, parser)
        url = f.geturl()
        f.close()
    except urllib2.HTTPError, e:
        print >> sys.stderr, "HTTPError fetching", unicode(original_url), "which was:", unicode(e)
        continue
    except urllib2.URLError, e:
        print >> sys.stderr, "URLError fetching", unicode(original_url), "which was:", unicode(e)
        continue
    root = tree.getroot()
    if root is not None:
        titles = root.xpath('.//title')
        if titles:
            title = titles[0].text
            if not title:
                title = '[Title was empty]'
        else:
            title = '[No title was found]'
    else:
        title = '[Parsing as HTML failed]'
    title = title.strip()
    title = re.sub('\s+', ' ', title)
    entry = SimpleEntry(row['from_user'],
                        url,
                        title,
                        text,
                        created_at,
                        [])
    url_dictionary[url].append(entry)
    url_dictionary[url].sort( key=lambda se: se.updated )

all_urls = url_dictionary.keys()

if number_trigger and len(all_urls) < number_trigger:
    sys.exit(-1)

all_urls.sort( key=lambda u: url_dictionary[u][0].updated )

summary_text = ""

for u in all_urls:
    # If this URL had been reported before the period we're
    # considering, just ignore it:
    cursor.execute("SELECT * FROM links WHERE url = ? AND updated_at < ?",(u,since))
    rows = cursor.fetchall()
    if len(rows) > 0:
        continue
    mentions = url_dictionary[u]
    title = mentions[0].title
    title = decode_htmlentities(title)
    summary_text += title + "\n"
    summary_text += u + "\n"
    bookmarked_by = map( lambda m: m.user, mentions )
    summary_text += "  (bookmarked by: %s)\n" % ( ", ".join(bookmarked_by) )
    for m in mentions:
        cursor.execute("SELECT * FROM links WHERE url = ? AND user = ? AND updated_at = ?",
                       ( u, m.user, m.updated ))
        rows = cursor.fetchall()
        if len(rows) == 0:
            cursor.execute("INSERT INTO links ( url, user, updated_at ) VALUES ( ?, ?, ? )", ( u, m.user, m.updated ))
            connection.commit()
        s = m.summary.strip()
        s = decode_htmlentities(s)
        if len(s) == 0:
            continue
        summary_text += "  %s says:\n" % m.user
        lines = textwrap.wrap(s,64)
        for l in lines:
            summary_text += "    %s\n" % l
        if m.tags:
            summary_text += "  ["+(', '.join(m.tags))+"]\n"
    summary_text += "\n"

print summary_text.encode('UTF-8')

if number_trigger:
    update_last_sent()
