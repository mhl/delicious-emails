#!/usr/bin/env python

# This script produces a plain text (UTF-8 encoded) summary of recent
# links posted to delicious.com by the users listed (one per line) in
# the file specified as the first argument.  You can either report all
# those in the last 7 days, or, if <NUMBER-TRIGGER> is supplied,
# produced empty output unless more than <NUMBER-TRIGGER> links have
# been added since the last time the script did produce output in this
# mode.

# Copyright Mark Longair 2008, 2009, 2010

import feedparser
import textwrap
import datetime
from htmlentitydefs import name2codepoint
import re
from pysqlite2 import dbapi2 as sqlite
import os
import sys
import json
import urllib2
from cookielib import CookieJar
from urlparse import urlparse
from lxml import etree
from collections import defaultdict
from twython import Twython
import dateutil.parser
import pytz
from optparse import OptionParser

utc = pytz.UTC

with open(os.path.join(os.environ['HOME'],
                       '.twitter-notesandtheories')) as fp:
    twitter_oauth = json.load(fp)

# We need a cookie jar, or opening t.co links results in an infinite
# 302 redirection loop:
cj = CookieJar()
url_opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))

# An sqlite database that keeps track of which links have already been
# reported, and when the triggering number of new links was last hit:
db_filename = os.path.join( os.environ['HOME'], "delicious-links.db" )

connection = sqlite.connect(db_filename)
cursor = connection.cursor()

cursor.execute("CREATE TABLE IF NOT EXISTS links ( url text, user text, updated_at timestamp )")
cursor.execute("CREATE TABLE IF NOT EXISTS updates ( last_sent_at timestamp )")

parser = OptionParser()
parser.add_option("-n", "--number-trigger",
                  dest="number_trigger",
                  type="int",
                  help="Only output and update if there have been > this number of new URLs")
parser.add_option("--dry-run",
                  dest="dry_run",
                  default=False, action="store_true",
                  help="Don't update the last sent time")
parser.add_option("-v", "--verbose",
                  dest="dry_run",
                  default=False, action="store_true",
                  help="Output verbose progress to STDERR")
parser.usage = "%prog [options] <ACCOUNTS-JSON-FILE>"
parser.description = """If --number-trigger is specified, output is
only produced if there has beeen more than that number of new
bookmarks since the last time that limit was exceeded.  Storing the
time when it was exceeded can be suppressed with the --dry-run
option."""

(options, args) = parser.parse_args()

if len(args) != 1:
    print >> sys.stderr, "You must supply an accounts filename"
    parser.print_help()
    sys.exit(1)

with open(args[0]) as f:
    accounts = json.load(f)

# Only report bookmarks that have been updated since 'since'.  By
# default this is seven days ...
since = datetime.datetime.now(utc) - datetime.timedelta(days=7)

if options.number_trigger:
    # ... or if we're using a number trigger, make it the last time
    # the trigger was reached:
    cursor.execute("SELECT last_sent_at FROM updates ORDER BY last_sent_at DESC LIMIT 1")
    row = cursor.fetchone()
    if row:
        since = utc.localize(datetime.datetime.strptime( row[0], "%Y-%m-%d %H:%M:%S" ))

def update_last_sent():
    cursor.execute("INSERT INTO updates (last_sent_at) VALUES (CURRENT_TIMESTAMP)")
    connection.commit()

# These next two useful functions are from:
#   http://snippets.dzone.com/posts/show/4569

def substitute_entity(match):
    ent = match.group(2)
    if match.group(1) == "#":
        return unichr(int(ent))
    else:
        cp = name2codepoint.get(ent)
        if cp:
            return unichr(cp)
        else:
            return match.group()

def decode_htmlentities(string):
    entity_re = re.compile("&(#?)(\d{1,5}|\w{1,8});")
    return entity_re.subn(substitute_entity, string)[0]

class DeliciousFeed:
    def __init__(self,user):
        self.user = user
        self.feed_url = "http://feeds.delicious.com/v2/rss/%s?count=100&plain" % user
        self.parsed = feedparser.parse(self.feed_url)
    def __str__(self):
        return self.feed_url
    # Add each entry to the global URL dictionary:
    def add_entries_to_url_dictionary(self,since,dictionary):
        d = dictionary
        for e in self.parsed['entries']:
            if not e['updated_parsed']:
               print >> sys.stderr, "An entry in %s had no updated_parsed field - pubDate might be empty" % (self.feed_url)
               continue
            updated = utc.localize(datetime.datetime(*(e['updated_parsed'][0:7])))
            if updated > since:
                url = e['link']
                summary = e.get('summary','')
                tags = None
                if 'tags' in e:
                    tags = [ x['term'] for x in e['tags'] if x['term'] and not re.search('^system:',x['term']) ]
                # For some reason I don't understand, sometimes I get
                # errors about there being no entry for 'author':
                author = e.get('author','[author missing]')
                title = e.get('title','[title missing]')
                d[url].append( SimpleEntry(author,url,title,summary,updated,tags ) )
                d[url].sort( key=lambda se: se.updated )

class SimpleEntry:
    def __init__(self,user,url,title,summary,updated,tags):
        self.user = user
        self.url = url
        self.title = title
        self.summary = summary
        self.updated = updated
        self.tags = tags

url_dictionary = defaultdict(list)

for u in accounts['delicious']:
    if options.verbose:
        print >> sys.stderr, "Considering Delicious account:", u
    feed = DeliciousFeed(u)
    feed.add_entries_to_url_dictionary(since,url_dictionary)

# Also get everyone's tweets from the twitter API:

twitter = Twython(twitter_oauth['consumer_key'],
                  access_token=twitter_oauth['access_token'])

for twitter_username in accounts['twitter']:
    if options.verbose:
        print >> sys.stderr, "Considering Twitter account:", twitter_username
    for tweet in twitter.get_user_timeline(screen_name=twitter_username,
                                           count=200):
        text = tweet['text'].strip()
        text = re.sub('\s+', ' ', text)
        if "http://" not in text and "https://" not in text:
            continue
        if text.startswith('@'):
            continue
        tweet_id = tweet['id']
        created_at = dateutil.parser.parse(tweet['created_at'])
        if created_at < since:
            continue
        first_url_match = re.search('https?://\S+', text)
        if not first_url_match:
            print >> sys.stderr, "Failed to find a URL in: ", text
            continue
        original_url = first_url_match.group(0)
        # If there's a trailing close-banana, then probably someone's just
        # put a URL in parentheses.  Although if there's an open-banana
        # earlier in the URL  then it's probably intended to be part of the URL:
        if not '(' in original_url:
            original_url = original_url.rstrip(')')
        # Sometimes a non-ASCII character can end up in the URL, which
        # will cause a UnicodeEncodeError when trying to open the URL.
        # Check for that here:
        try:
            ascii_url = original_url.encode('ascii')
        except UnicodeEncodeError, e:
            print >> sys.stderr, "Found a non-ASCII character in the URL in Tweet with ID", tweet_id
            continue
        parser = etree.HTMLParser()
        try:
            f = url_opener.open(ascii_url)
            tree = etree.parse(f, parser)
            url = f.geturl()
            f.close()
        except urllib2.HTTPError, e:
            print >> sys.stderr, "HTTPError fetching", ascii_url, "which was:", unicode(e)
            continue
        except urllib2.URLError, e:
            print >> sys.stderr, "URLError fetching", ascii_url, "which was:", unicode(e)
            continue
        root = tree.getroot()
        if root is not None:
            titles = root.xpath('.//title')
            if titles:
                title = titles[0].text
                if not title:
                    title = '[Title was empty]'
            else:
                title = '[No title was found]'
        else:
            title = '[Parsing as HTML failed]'
        title = title.strip()
        title = re.sub('\s+', ' ', title)
        entry = SimpleEntry(twitter_username,
                            url,
                            title,
                            text,
                            created_at,
                            [])
        url_dictionary[url].append(entry)
        url_dictionary[url].sort( key=lambda se: se.updated )

all_urls = url_dictionary.keys()

if options.number_trigger and len(all_urls) < options.number_trigger:
    sys.exit(-1)

all_urls.sort( key=lambda u: url_dictionary[u][0].updated )

summary_text = ""

for u in all_urls:
    # If this URL had been reported before the period we're
    # considering, just ignore it:
    cursor.execute("SELECT * FROM links WHERE url = ? AND updated_at < ?",(u,since))
    rows = cursor.fetchall()
    if len(rows) > 0:
        continue
    mentions = url_dictionary[u]
    title = mentions[0].title
    title = decode_htmlentities(title)
    summary_text += title + "\n"
    summary_text += u + "\n"
    bookmarked_by = map( lambda m: m.user, mentions )
    summary_text += "  (bookmarked by: %s)\n" % ( ", ".join(bookmarked_by) )
    for m in mentions:
        cursor.execute("SELECT * FROM links WHERE url = ? AND user = ? AND updated_at = ?",
                       ( u, m.user, m.updated ))
        rows = cursor.fetchall()
        if len(rows) == 0:
            cursor.execute("INSERT INTO links ( url, user, updated_at ) VALUES ( ?, ?, ? )", ( u, m.user, m.updated ))
            connection.commit()
        s = m.summary.strip()
        s = decode_htmlentities(s)
        if len(s) == 0:
            continue
        summary_text += "  %s says:\n" % m.user
        lines = textwrap.wrap(s,64)
        for l in lines:
            summary_text += "    %s\n" % l
        if m.tags:
            summary_text += "  ["+(', '.join(m.tags))+"]\n"
    summary_text += "\n"

print summary_text.encode('UTF-8')

if options.number_trigger and not options.dry_run:
    update_last_sent()
